---
title: Resumen 93.53 - Metodos Numericos
author: Agustin Fisher

---

# Sistemas de Ecuaciones Lineales

Un sistema de ecuaciones lineales es de la forma $$ a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n = b_1$$
$$ a_{21}x_1 + a_{22}x_2 + ... + a_{2n}x_n = b_2 $$
hasta
$$ a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}x_n = b_m $$

Este sistema se puede escribir en forma abreviada y matricial como

$$ AX = B $$

Buscamos hallar la solucion a traves de **metodos directos**, que buscan hallar la solucion exacta o **metodos indirectos o iterativos** que buscan hallar una solucion aproximada.

**Ejemplo:** supongamos que $A$ es cuadrada e inversible. Entonces $AX=B \Rightarrow X=A^{-1}B$, donde $A^{-1}=\frac{adj(A)}{det(A)}$. Vamos a tener que idear estrategias que nos permitan minimizar el costo computacional de resolver el sistema.

## Sistemas Desacoplados

Este es el caso mas facil de resolver. Es el que se da cuando cada una de las ecuaciones presenta a una variable distinta, no relacionada a las otras. La matriz que representa a este sistema resulta ser una diagonal cuadrada.

Aca es facil ver que para calcular el valor de cada una de las variables simplemente hay que realizar una division.
$$ x_m = \frac{b_n}{a_nn} $$

Entonces el costo operacional es de $n$ operaciones, es decir $O(n)$.

## Sistema Triangular

Es el que se da cuando la matriz que representa al sistema de ecuaciones es triangular. Todo lo que este de un lado de la diagonal es cero, y del otro contiene coeficientes.

La manera de resolver este sistema es con lo que se conoce como una sustitucion regresiva o progresiva. Se resuelve lo posible desde el lado de ceros, y se reemplazan esos resultados en el triangulo.

Esto se suele representar con una $L$ de *Lower* o una $U$ de *Upper*.
$$ L\vec{x} = \vec{B} $$

La complejidad algoritmica es mayor que para el caso anterior pero no es de gran interes. La situacion de lower se resuelve con una sustitucion hacia adelante, o progresiva, y el caso de upper con una sustitucion hacia atras o regresiva. En general,

$$ x_n = \frac{b_n - a_{n1}x_1 - a_{n2}x_2 - ... - a_{n(n-1)x_{n-1}}}{a_{nn}}$$

Vamos a estar realizando, para una matriz cuadrada $k\times k$ unos $k-1$ productos, $k-1$ sumas y una division en cada paso. Haciendo la sumatoria de esto para todos los pasos, resulta una complejidad de $O(n^2)$. La demostracion de esta complejidad esta por fuera de esta materia, pero no es complicada.

### Sistemas Triangulares: Errores

Definamos a $|F|$ cuyos elementos son los valores absolutos de los elementos de $F$. Entonces $|F| \leq |G| \Leftrightarrow |f_{ij} \leq g_{ij}|$.

Para indicar el error en una matriz, lo podemos introducir como una perturbacion. Entonces

$$ L\vec{x}=\vec{n} \to (L+F)\vec{x}=\vec{b} \text{ y } |F| \leq n\frac{\epsilon_m}{2}|L| + O(\epsilon_m^2) $$

## Sistemas Completos

En caso de que el sistema no sea triangular... lo hacemos triangular! Esto se hace por medio de la eliminacion gaussiana. Una manera algoritmicamente mas practica es operar entre filas para obtener lo que uno quiere.

Se puede mostrar que la resolucion de un sistema como este es $O(\frac{2}{3}n^3)$

# Matriz Inversa

Esto se resuelve con una aplicacion de la eliminacion de Gauss. Se toma la matriz original y se le adjunta la matriz identidad. Luego se hace eliminacion de Gauss hasta que la original se convierta en la identidad, la que resulte como adjunta, sera la matriz inversa.

$$ (A|I) ~ (I|A^{-1}) $$

El calculo de la matriz invera lleva $n^3$ operaciones. Pero, salvo que sea necesario por otro motivo, hay formas mas eficientes de proceder.

# Factorizacion LU

La idea es tomar una matriz $A$ y escribirla como el producto de dos matrices $L$ y $U$.

$$A = LU$$

Entonces, una ecuacion se puede reescribir como

$$ AX=B \Rightarrow LUX=B \Rightarrow UX=L^{-1}B  \Rightarrow X=U^{-1}L^{-1}B$$

Voy a repasar alguna que otra propiedad de algebra lineal que puede ser util para encarar el resto de estos temas.

$$ A=LU \Rightarrow \det{(A)} = \det{(L)}\det{(U)} = \prod_{i=1}^n( l_{ii})\prod_{j=1}^n(u_{jj}) $$

Esta factorizacion lleva $\frac{2}{3}n^3$ operaciones.
